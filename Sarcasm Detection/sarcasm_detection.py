# -*- coding: utf-8 -*-
"""Sarcasm-Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b8ZxPt39QVSsBNh93u4Smlv8qhZ9s92u

# NLP - Sarcasm Detection
MLP Model + **Embedding** Layer
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import TextVectorization
from tensorflow.keras import Sequential
from tensorflow.keras import layers
from tensorflow import keras
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

"""## 1- Load Data"""

!wget --no-check-certificate https://storage.googleapis.com/learning-datasets/sarcasm.json

df = pd.read_json('sarcasm.json')

df.head()

df.shape

sentences = df['headline'].to_list()
labels = df['is_sarcastic'].to_list()

"""## 2- Pre-processing"""

sw = stopwords.words('english')

print(sw)

for s in sentences[:2]:
  print(s)

for i in range(len(sentences)):
  words = sentences[i].split()
  words_new = [w for w in words if w not in sw]
  sentences[i] = ' '.join(words_new)

for s in sentences[:2]:
  print(s)

training_size = 23000

train_sentences = sentences[:training_size]
validation_sentences = sentences[training_size:]

train_labels = labels[:training_size]
validation_labels = labels[training_size:]

train_labels = np.array(train_labels)
validation_labels = np.array(validation_labels)

vocab_size = 5000
embedding_dim = 16

text_vectorization = TextVectorization(max_tokens=vocab_size, output_sequence_length=25)
text_vectorization.adapt(train_sentences)
print(len(text_vectorization.get_vocabulary()))

train_sequences = text_vectorization(train_sentences)
validation_sequences = text_vectorization(validation_sentences)
print(type(train_sequences))
print(train_sequences.shape)
print(validation_sequences.shape)

"""## 3- Model Design"""

model1 = Sequential([
    layers.Embedding(vocab_size, embedding_dim),
    layers.GlobalAveragePooling1D(),
    layers.Dense(25, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

model1.summary()

"""## 4- Training"""

adam = keras.optimizers.Adam(learning_rate=0.0001)
model1.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])

history1 = model1.fit(train_sequences, train_labels,
                      batch_size=128,
                      epochs=50,
                      validation_data=(validation_sequences, validation_labels))

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(history1, "accuracy")
plot_graphs(history1, "loss")

test_sentences = ["granny starting to fear spiders in the garden might be real",
                  "game of thrones season finale showing this sunday night",
                  "TensorFlow book will be a best seller"]
# Remove stop-words
test_sequences = text_vectorization(test_sentences)
print(test_sequences)

preds = model1.predict(test_sequences)
print(preds)
